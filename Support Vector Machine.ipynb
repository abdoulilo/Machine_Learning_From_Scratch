{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ce8d8a9b",
   "metadata": {},
   "source": [
    "SVM is a supervised learning algorithm used for classification (and regression).\n",
    "The goal is simple: find the best boundary (hyperplane) that separates data points of different classes with the largest possible margin.\n",
    "\n",
    "Hyperplane: A decision boundary. In 2D it’s a line, in 3D a plane, and in higher dimensions, a hyperplane.\n",
    "\n",
    "Margin: The distance between the hyperplane and the closest data points (support vectors).\n",
    "\n",
    "Support Vectors: The data points that lie closest to the decision boundary. They “support” the hyperplane.\n",
    "\n",
    "The intuition: the larger the margin, the better the generalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b0c87680",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Callable, Tuple, Optional, Dict\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b92835e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = {\n",
    "    \"X\": [\n",
    "        [2.4967, 1.8617],\n",
    "        [2.6477, 3.5230],\n",
    "        [1.7658, 1.7659],\n",
    "        [3.5792, 2.7674],\n",
    "        [1.5305, 2.5426],\n",
    "        [1.5366, 1.5343],\n",
    "        [2.2420, 0.0867],\n",
    "        [0.2751, 1.4377],\n",
    "        [0.9872, 2.3142],\n",
    "        [1.0920, 0.5877],\n",
    "        [-0.5344, -2.2258],\n",
    "        [-1.9325, -3.4247],\n",
    "        [-2.5444, -1.8891],\n",
    "        [-3.1510, -1.6243],\n",
    "        [-2.6006, -2.2917],\n",
    "        [-2.6017, -0.1477],\n",
    "        [-2.0135, -3.0577],\n",
    "        [-1.1775, -3.2208],\n",
    "        [-1.7911, -3.9597],\n",
    "        [-3.3282, -1.8031]\n",
    "    ],\n",
    "    \"y\": [\n",
    "        1.0, 1.0, 1.0, 1.0, 1.0, \n",
    "        1.0, 1.0, 1.0, 1.0, 1.0, \n",
    "        -1.0, -1.0, -1.0, -1.0, -1.0, \n",
    "        -1.0, -1.0, -1.0, -1.0, -1.0\n",
    "    ]\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "51fccbf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decision_score(x: np.ndarray, w: np.ndarray, b: float) -> float:\n",
    "    \"\"\"Compute linear decision score f(x) = w^T x + b.\"\"\"\n",
    "    return float(np.dot(w, x) + b)\n",
    "\n",
    "def predict_label(x: np.ndarray, w: np.ndarray, b: float) -> int:\n",
    "    \"\"\"Predict {-1, +1} using sign(w^T x + b).\"\"\"\n",
    "    return 1 if decision_score(x, w, b) >= 0.0 else -1\n",
    "\n",
    "def batch_predict(X: np.ndarray, w: np.ndarray, b: float) -> np.ndarray:\n",
    "    \"\"\"Vectorized prediction for matrix X (n x d).\"\"\"\n",
    "    scores = X @ w + b\n",
    "    return np.where(scores >= 0.0, 1, -1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "39ee59b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hinge_loss_terms(X: np.ndarray, y: np.ndarray, w: np.ndarray, b: float) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Return per-sample hinge losses: max(0, 1 - y_i * (w^T x_i + b)).\n",
    "    X: (n, d), y: (n,), w: (d,), b: float\n",
    "    \"\"\"\n",
    "    margins = 1.0 - y * (X @ w + b)\n",
    "    return np.maximum(0.0, margins)\n",
    "\n",
    "def objective_primal(X: np.ndarray, y: np.ndarray, w: np.ndarray, b: float, lam: float) -> float:\n",
    "    \"\"\"\n",
    "    Regularized hinge loss objective:\n",
    "    J(w,b) = (lam/2) * ||w||^2 + (1/n) * sum_i max(0, 1 - y_i (w^T x_i + b))\n",
    "    \"\"\"\n",
    "    n = X.shape[0]\n",
    "    reg = 0.5 * lam * np.dot(w, w)\n",
    "    loss = hinge_loss_terms(X, y, w, b).mean()\n",
    "    return reg + loss\n",
    "\n",
    "def subgradients_primal(X: np.ndarray, y: np.ndarray, w: np.ndarray, b: float, lam: float) -> Tuple[np.ndarray, float]:\n",
    "    \"\"\"\n",
    "    Subgradients of J(w,b) w.r.t. (w, b).\n",
    "    \"\"\"\n",
    "    n = X.shape[0]\n",
    "    margins = y * (X @ w + b)\n",
    "    mask = margins < 1.0  # misclassified or on margin\n",
    "    if np.any(mask):\n",
    "        # Average over all samples (full-batch)\n",
    "        g_w = lam * w - (X[mask] * y[mask, None]).mean(axis=0)\n",
    "        g_b = -y[mask].mean()\n",
    "    else:\n",
    "        g_w = lam * w\n",
    "        g_b = 0.0\n",
    "    return g_w, float(g_b)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c62af6b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearSVMPrimal:\n",
    "    \"\"\"\n",
    "    Linear SVM trained by full-batch subgradient descent on the primal objective.\n",
    "    Good as a baseline. No kernels. \n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, lam: float = 1e-2, max_iters: int = 2000, stepsize: Optional[Callable[[int], float]] = None,\n",
    "                 shuffle: bool = True, random_state: Optional[int] = 42):\n",
    "        \"\"\"\n",
    "        lam: regularization coefficient (λ > 0)\n",
    "        stepsize: function t -> eta_t; default = 1 / (λ t)\n",
    "        \"\"\"\n",
    "        if lam <= 0:\n",
    "            raise ValueError(\"lam must be > 0\")\n",
    "        self.lam = lam\n",
    "        self.max_iters = max_iters\n",
    "        self.stepsize_fn = stepsize\n",
    "        self.shuffle = shuffle\n",
    "        self.random_state = random_state\n",
    "\n",
    "        self.w: Optional[np.ndarray] = None\n",
    "        self.b: float = 0.0\n",
    "        self.history: Dict[str, list] = {\"objective\": []}\n",
    "\n",
    "    def _default_stepsize(self, t: int) -> float:\n",
    "        return 1.0 / (self.lam * max(1, t))\n",
    "\n",
    "    def fit(self, X: np.ndarray, y: np.ndarray) -> \"LinearSVMPrimal\":\n",
    "        rng = np.random.default_rng(self.random_state)\n",
    "        n, d = X.shape\n",
    "        self.w = np.zeros(d, dtype=float)\n",
    "        self.b = 0.0\n",
    "\n",
    "        idx = np.arange(n)\n",
    "        for t in range(1, self.max_iters + 1):\n",
    "            if self.shuffle:\n",
    "                rng.shuffle(idx)\n",
    "                X_batch = X[idx]\n",
    "                y_batch = y[idx]\n",
    "            else:\n",
    "                X_batch = X\n",
    "                y_batch = y\n",
    "\n",
    "            g_w, g_b = subgradients_primal(X_batch, y_batch, self.w, self.b, self.lam)\n",
    "            eta = self.stepsize_fn(t) if self.stepsize_fn else self._default_stepsize(t)\n",
    "\n",
    "            self.w -= eta * g_w\n",
    "            self.b -= eta * g_b\n",
    "\n",
    "            obj = objective_primal(X, y, self.w, self.b, self.lam)\n",
    "            self.history[\"objective\"].append(obj)\n",
    "        return self\n",
    "\n",
    "    def decision_function(self, X: np.ndarray) -> np.ndarray:\n",
    "        if self.w is None:\n",
    "            raise RuntimeError(\"Model is not fitted.\")\n",
    "        return X @ self.w + self.b\n",
    "\n",
    "    def predict(self, X: np.ndarray) -> np.ndarray:\n",
    "        scores = self.decision_function(X)\n",
    "        return np.where(scores >= 0.0, 1, -1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "65d694e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearSVMPegasos:\n",
    "    \"\"\"\n",
    "    Pegasos: Stochastic Subgradient Descent for Linear SVM.\n",
    "    Efficient for large datasets. Includes optional projection step.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, lam: float = 1e-4, max_iters: int = 5000, batch_size: int = 32,\n",
    "                 project: bool = True, random_state: Optional[int] = 42):\n",
    "        if lam <= 0:\n",
    "            raise ValueError(\"lam must be > 0\")\n",
    "        self.lam = lam\n",
    "        self.max_iters = max_iters\n",
    "        self.batch_size = batch_size\n",
    "        self.project = project\n",
    "        self.random_state = random_state\n",
    "\n",
    "        self.w: Optional[np.ndarray] = None\n",
    "        self.b: float = 0.0\n",
    "        self.history: Dict[str, list] = {\"objective\": []}\n",
    "\n",
    "    def fit(self, X: np.ndarray, y: np.ndarray) -> \"LinearSVMPegasos\":\n",
    "        rng = np.random.default_rng(self.random_state)\n",
    "        n, d = X.shape\n",
    "        self.w = np.zeros(d, dtype=float)\n",
    "        self.b = 0.0\n",
    "\n",
    "        for t in range(1, self.max_iters + 1):\n",
    "            # Sample minibatch\n",
    "            batch_idx = rng.choice(n, size=min(self.batch_size, n), replace=False)\n",
    "            Xb = X[batch_idx]\n",
    "            yb = y[batch_idx]\n",
    "\n",
    "            # Stochastic subgradient\n",
    "            margins = yb * (Xb @ self.w + self.b)\n",
    "            mask = margins < 1.0\n",
    "\n",
    "            g_w = self.lam * self.w\n",
    "            g_b = 0.0\n",
    "            if np.any(mask):\n",
    "                g_w = g_w - (Xb[mask] * yb[mask, None]).mean(axis=0)\n",
    "                g_b = g_b - yb[mask].mean()\n",
    "\n",
    "            eta = 1.0 / (self.lam * t)\n",
    "            self.w -= eta * g_w\n",
    "            self.b -= eta * g_b\n",
    "\n",
    "            # Optional projection step: keep ||w|| <= 1/sqrt(lam)\n",
    "            if self.project:\n",
    "                norm_w = np.linalg.norm(self.w)\n",
    "                cap = 1.0 / math.sqrt(self.lam)\n",
    "                if norm_w > cap:\n",
    "                    self.w *= (cap / norm_w)\n",
    "\n",
    "            # Track objective on full data (optional; costs O(n d))\n",
    "            obj = objective_primal(X, y, self.w, self.b, self.lam)\n",
    "            self.history[\"objective\"].append(obj)\n",
    "\n",
    "        return self\n",
    "\n",
    "    def decision_function(self, X: np.ndarray) -> np.ndarray:\n",
    "        if self.w is None:\n",
    "            raise RuntimeError(\"Model is not fitted.\")\n",
    "        return X @ self.w + self.b\n",
    "\n",
    "    def predict(self, X: np.ndarray) -> np.ndarray:\n",
    "        scores = self.decision_function(X)\n",
    "        return np.where(scores >= 0.0, 1, -1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1b3306f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def linear_kernel(a: np.ndarray, b: np.ndarray) -> float:\n",
    "    return float(a @ b)\n",
    "\n",
    "def poly_kernel(a: np.ndarray, b: np.ndarray, degree: int = 3, coef0: float = 1.0, gamma: Optional[float] = None) -> float:\n",
    "    if gamma is None:\n",
    "        gamma = 1.0 / a.shape[0]\n",
    "    return float((gamma * (a @ b) + coef0) ** degree)\n",
    "\n",
    "def rbf_kernel(a: np.ndarray, b: np.ndarray, gamma: Optional[float] = None) -> float:\n",
    "    if gamma is None:\n",
    "        gamma = 1.0 / a.shape[0]  # simple default\n",
    "    diff = a - b\n",
    "    return float(np.exp(-gamma * (diff @ diff)))\n",
    "\n",
    "\n",
    "class KernelSVM_SMO:\n",
    "    \"\"\"\n",
    "    Soft-margin SVM trained with SMO in the dual.\n",
    "    Supports custom kernels (linear, poly, RBF).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, C: float = 1.0, kernel: Callable[[np.ndarray, np.ndarray], float] = None,\n",
    "                 tol: float = 1e-3, max_passes: int = 5, random_state: Optional[int] = 42):\n",
    "        if C <= 0:\n",
    "            raise ValueError(\"C must be > 0\")\n",
    "        self.C = C\n",
    "        self.kernel = kernel if kernel is not None else rbf_kernel\n",
    "        self.tol = tol\n",
    "        self.max_passes = max_passes\n",
    "        self.random_state = random_state\n",
    "\n",
    "        # Learned parameters\n",
    "        self.alpha: Optional[np.ndarray] = None\n",
    "        self.b: float = 0.0\n",
    "        self.X: Optional[np.ndarray] = None\n",
    "        self.y: Optional[np.ndarray] = None\n",
    "        self._K: Optional[np.ndarray] = None  # Gram matrix (n x n)\n",
    "\n",
    "    def _compute_kernel_matrix(self, X: np.ndarray) -> np.ndarray:\n",
    "        n = X.shape[0]\n",
    "        K = np.empty((n, n), dtype=float)\n",
    "        for i in range(n):\n",
    "            for j in range(i, n):\n",
    "                val = self.kernel(X[i], X[j])\n",
    "                K[i, j] = val\n",
    "                K[j, i] = val\n",
    "        return K\n",
    "\n",
    "    def _f(self, i: int) -> float:\n",
    "        \"\"\"Decision score at training point i: f(x_i).\"\"\"\n",
    "        return float(np.sum(self.alpha * self.y * self._K[:, i]) + self.b)\n",
    "\n",
    "    def fit(self, X: np.ndarray, y: np.ndarray) -> \"KernelSVM_SMO\":\n",
    "        rng = np.random.default_rng(self.random_state)\n",
    "        n = X.shape[0]\n",
    "\n",
    "        self.X = X.astype(float, copy=True)\n",
    "        self.y = y.astype(float, copy=True)\n",
    "        self.alpha = np.zeros(n, dtype=float)\n",
    "        self.b = 0.0\n",
    "        self._K = self._compute_kernel_matrix(self.X)\n",
    "\n",
    "        passes = 0\n",
    "        while passes < self.max_passes:\n",
    "            num_changed = 0\n",
    "            for i in range(n):\n",
    "                E_i = self._f(i) - self.y[i]\n",
    "                KKT_viol = ((self.y[i] * E_i < -self.tol and self.alpha[i] < self.C) or\n",
    "                            (self.y[i] * E_i >  self.tol and self.alpha[i] > 0))\n",
    "                if not KKT_viol:\n",
    "                    continue\n",
    "\n",
    "                # Choose j != i (simple heuristic: random)\n",
    "                j = i\n",
    "                while j == i:\n",
    "                    j = rng.integers(0, n)\n",
    "                E_j = self._f(j) - self.y[j]\n",
    "\n",
    "                alpha_i_old = self.alpha[i]\n",
    "                alpha_j_old = self.alpha[j]\n",
    "\n",
    "                # Compute L, H\n",
    "                if self.y[i] != self.y[j]:\n",
    "                    L = max(0.0, self.alpha[j] - self.alpha[i])\n",
    "                    H = min(self.C, self.C + self.alpha[j] - self.alpha[i])\n",
    "                else:\n",
    "                    L = max(0.0, self.alpha[i] + self.alpha[j] - self.C)\n",
    "                    H = min(self.C, self.alpha[i] + self.alpha[j])\n",
    "\n",
    "                if L == H:\n",
    "                    continue\n",
    "\n",
    "                # eta = 2 K_ij - K_ii - K_jj\n",
    "                eta = 2.0 * self._K[i, j] - self._K[i, i] - self._K[j, j]\n",
    "                if eta >= 0:\n",
    "                    continue\n",
    "\n",
    "                # Update alpha[j]\n",
    "                self.alpha[j] = self.alpha[j] - (self.y[j] * (E_i - E_j)) / eta\n",
    "                # Clip to [L, H]\n",
    "                if self.alpha[j] > H:\n",
    "                    self.alpha[j] = H\n",
    "                elif self.alpha[j] < L:\n",
    "                    self.alpha[j] = L\n",
    "\n",
    "                # Check for significant change\n",
    "                if abs(self.alpha[j] - alpha_j_old) < 1e-12:\n",
    "                    self.alpha[j] = alpha_j_old\n",
    "                    continue\n",
    "\n",
    "                # Update alpha[i]\n",
    "                self.alpha[i] = self.alpha[i] + self.y[i] * self.y[j] * (alpha_j_old - self.alpha[j])\n",
    "\n",
    "                # Compute b1, b2\n",
    "                b1 = (self.b - E_i\n",
    "                      - self.y[i] * (self.alpha[i] - alpha_i_old) * self._K[i, i]\n",
    "                      - self.y[j] * (self.alpha[j] - alpha_j_old) * self._K[i, j])\n",
    "\n",
    "                b2 = (self.b - E_j\n",
    "                      - self.y[i] * (self.alpha[i] - alpha_i_old) * self._K[i, j]\n",
    "                      - self.y[j] * (self.alpha[j] - alpha_j_old) * self._K[j, j])\n",
    "\n",
    "                # Update b\n",
    "                if 0 < self.alpha[i] < self.C:\n",
    "                    self.b = b1\n",
    "                elif 0 < self.alpha[j] < self.C:\n",
    "                    self.b = b2\n",
    "                else:\n",
    "                    self.b = 0.5 * (b1 + b2)\n",
    "\n",
    "                num_changed += 1\n",
    "\n",
    "            passes = passes + 1 if num_changed == 0 else 0\n",
    "\n",
    "        return self\n",
    "\n",
    "    def decision_function(self, X: np.ndarray) -> np.ndarray:\n",
    "        if self.alpha is None or self.X is None or self.y is None:\n",
    "            raise RuntimeError(\"Model is not fitted.\")\n",
    "        n_train = self.X.shape[0]\n",
    "        n_eval = X.shape[0]\n",
    "        scores = np.zeros(n_eval, dtype=float)\n",
    "        for i in range(n_eval):\n",
    "            s = 0.0\n",
    "            xi = X[i]\n",
    "            for j in range(n_train):\n",
    "                if self.alpha[j] > 0.0:\n",
    "                    s += self.alpha[j] * self.y[j] * self.kernel(self.X[j], xi)\n",
    "            scores[i] = s + self.b\n",
    "        return scores\n",
    "\n",
    "    def predict(self, X: np.ndarray) -> np.ndarray:\n",
    "        scores = self.decision_function(X)\n",
    "        return np.where(scores >= 0.0, 1, -1)\n",
    "\n",
    "    def support_indices(self, tol: float = 1e-10) -> np.ndarray:\n",
    "        \"\"\"Indices with alpha > tol.\"\"\"\n",
    "        if self.alpha is None:\n",
    "            return np.array([], dtype=int)\n",
    "        return np.flatnonzero(self.alpha > tol)\n",
    "\n",
    "    def linear_w_if_applicable(self) -> Optional[np.ndarray]:\n",
    "        \"\"\"\n",
    "        If kernel is linear, recover w = sum_i alpha_i y_i x_i.\n",
    "        Otherwise returns None.\n",
    "        \"\"\"\n",
    "        if self.alpha is None or self.X is None or self.y is None:\n",
    "            return None\n",
    "        # heuristically check: if kernel evaluated as dot\n",
    "        try:\n",
    "            # Compare a few evaluations to dot product to decide\n",
    "            # (not foolproof; use only if you know it's linear_kernel)\n",
    "            return (self.alpha * self.y) @ self.X\n",
    "        except Exception:\n",
    "            return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e1545344",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Primal] Train accuracy: 1.000\n",
      "[Pegasos] Train accuracy: 1.000\n",
      "[SMO-RBF] Train accuracy: 1.000\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    rng = np.random.default_rng(42)\n",
    "    n_per = 20\n",
    "    X_pos = rng.normal(loc=2.0, scale=1.0, size=(n_per, 2))\n",
    "    X_neg = rng.normal(loc=-2.0, scale=1.0, size=(n_per, 2))\n",
    "    X = np.vstack([X_pos, X_neg])\n",
    "    y = np.hstack([np.ones(n_per), -np.ones(n_per)])\n",
    "\n",
    "    # --- Linear (Primal) ---\n",
    "    primal = LinearSVMPrimal(lam=1e-2, max_iters=2000)\n",
    "    primal.fit(X, y)\n",
    "    pred_primal = primal.predict(X)\n",
    "    acc_primal = (pred_primal == y).mean()\n",
    "    print(f\"[Primal] Train accuracy: {acc_primal:.3f}\")\n",
    "\n",
    "    # --- Linear (Pegasos) ---\n",
    "    pegasos = LinearSVMPegasos(lam=1e-4, max_iters=4000, batch_size=16)\n",
    "    pegasos.fit(X, y)\n",
    "    pred_peg = pegasos.predict(X)\n",
    "    acc_peg = (pred_peg == y).mean()\n",
    "    print(f\"[Pegasos] Train accuracy: {acc_peg:.3f}\")\n",
    "\n",
    "    # --- Kernel (SMO, RBF) ---\n",
    "    smo = KernelSVM_SMO(C=1.0, kernel=lambda a, b: rbf_kernel(a, b, gamma=0.5),\n",
    "                        tol=1e-3, max_passes=5)\n",
    "    smo.fit(X, y)\n",
    "    pred_smo = smo.predict(X)\n",
    "    acc_smo = (pred_smo == y).mean()\n",
    "    print(f\"[SMO-RBF] Train accuracy: {acc_smo:.3f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
