{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c0252204",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "linear_regression_scratch.py\n",
    "----------------------------\n",
    "A scikit-learn-like Linear Regression implemented from scratch using NumPy.\n",
    "\n",
    "Features\n",
    "- API similar to sklearn: fit(X, y), predict(X), score(X, y), get_params(), set_params()\n",
    "- Solvers: \"normal\" (closed-form) or \"gd\" (gradient descent)\n",
    "- Optional L2 regularization (ridge)\n",
    "- fit_intercept handling (intercept is never regularized)\n",
    "- Optional standardization for GD stability\n",
    "- Metrics: mse, mae, r2_score\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import annotations\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Optional, Dict, Any, Tuple\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e1382476",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- Metrics ----------\n",
    "def mse(y_true: np.ndarray, y_pred: np.ndarray) -> float:\n",
    "    y_true = np.asarray(y_true).reshape(-1)\n",
    "    y_pred = np.asarray(y_pred).reshape(-1)\n",
    "    return float(np.mean((y_true - y_pred) ** 2))\n",
    "\n",
    "\n",
    "def mae(y_true: np.ndarray, y_pred: np.ndarray) -> float:\n",
    "    y_true = np.asarray(y_true).reshape(-1)\n",
    "    y_pred = np.asarray(y_pred).reshape(-1)\n",
    "    return float(np.mean(np.abs(y_true - y_pred)))\n",
    "\n",
    "\n",
    "def r2_score(y_true: np.ndarray, y_pred: np.ndarray) -> float:\n",
    "    y_true = np.asarray(y_true).reshape(-1)\n",
    "    y_pred = np.asarray(y_pred).reshape(-1)\n",
    "    ss_res = float(np.sum((y_true - y_pred) ** 2))\n",
    "    ss_tot = float(np.sum((y_true - np.mean(y_true)) ** 2))\n",
    "    return 1.0 - ss_res / ss_tot if ss_tot != 0 else 0.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a006bff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- Utilities ----------\n",
    "@dataclass\n",
    "class _StandardScaler:\n",
    "    \"\"\"Minimal standard scaler for GD stability.\"\"\"\n",
    "    mean_: Optional[np.ndarray] = field(default=None, init=False)\n",
    "    std_: Optional[np.ndarray]  = field(default=None, init=False)\n",
    "    eps: float = 1e-12\n",
    "\n",
    "    def fit(self, X: np.ndarray) -> \"_StandardScaler\":\n",
    "        X = np.asarray(X, dtype=float)\n",
    "        self.mean_ = X.mean(axis=0)\n",
    "        self.std_  = X.std(axis=0)\n",
    "        # Prevent divide-by-zero for constant columns\n",
    "        self.std_[self.std_ < self.eps] = 1.0\n",
    "        return self\n",
    "\n",
    "    def transform(self, X: np.ndarray) -> np.ndarray:\n",
    "        if self.mean_ is None or self.std_ is None:\n",
    "            raise RuntimeError(\"Scaler not fitted.\")\n",
    "        X = np.asarray(X, dtype=float)\n",
    "        return (X - self.mean_) / self.std_\n",
    "\n",
    "    def fit_transform(self, X: np.ndarray) -> np.ndarray:\n",
    "        return self.fit(X).transform(X)\n",
    "\n",
    "\n",
    "def _check_X_y(X: np.ndarray, y: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    X = np.asarray(X, dtype=float)\n",
    "    y = np.asarray(y, dtype=float).reshape(-1)\n",
    "    if X.ndim == 1:\n",
    "        X = X.reshape(-1, 1)\n",
    "    if X.shape[0] != y.shape[0]:\n",
    "        raise ValueError(f\"X and y have incompatible shapes: {X.shape} vs {y.shape}\")\n",
    "    return X, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c5e42486",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- Model ----------\n",
    "class LinearRegression:\n",
    "    \"\"\"\n",
    "    Linear Regression (from scratch) with a scikit-learn-like interface.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    fit_intercept : bool, default=True\n",
    "        Whether to fit the intercept term.\n",
    "    solver : {\"normal\", \"gd\"}, default=\"normal\"\n",
    "        \"normal\": closed-form solution (optionally ridge via l2 > 0).\n",
    "        \"gd\": gradient descent (useful for large data or to mimic iterative solvers).\n",
    "    l2 : float, default=0.0\n",
    "        L2 regularization strength (ridge). Intercept is never regularized.\n",
    "    learning_rate : float, default=1e-2\n",
    "        Step size for gradient descent (used only if solver=\"gd\").\n",
    "    n_iters : int, default=10_000\n",
    "        Max iterations for gradient descent.\n",
    "    tol : float, default=1e-8\n",
    "        Early-stopping tolerance for GD based on loss improvement.\n",
    "    standardize : bool, default=False\n",
    "        If True and solver=\"gd\", standardizes features internally for stability.\n",
    "        Predictions are automatically unscaled; users never see scaled features.\n",
    "    random_state : Optional[int], default=None\n",
    "        Seed for GD weight initialization.\n",
    "    verbose : bool, default=False\n",
    "        If True, prints convergence info for GD.\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    coef_ : np.ndarray of shape (n_features,)\n",
    "        Fitted coefficients (weights) for features.\n",
    "    intercept_ : float\n",
    "        Fitted intercept.\n",
    "    loss_history_ : list[float]\n",
    "        Loss values per GD iteration (for solver=\"gd\").\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        fit_intercept: bool = True,\n",
    "        solver: str = \"normal\",\n",
    "        l2: float = 0.0,\n",
    "        learning_rate: float = 1e-2,\n",
    "        n_iters: int = 10_000,\n",
    "        tol: float = 1e-8,\n",
    "        standardize: bool = False,\n",
    "        random_state: Optional[int] = None,\n",
    "        verbose: bool = False,\n",
    "    ):\n",
    "        if solver not in {\"normal\", \"gd\"}:\n",
    "            raise ValueError(\"solver must be 'normal' or 'gd'\")\n",
    "        self.fit_intercept = fit_intercept\n",
    "        self.solver = solver\n",
    "        self.l2 = float(l2)\n",
    "        self.learning_rate = float(learning_rate)\n",
    "        self.n_iters = int(n_iters)\n",
    "        self.tol = float(tol)\n",
    "        self.standardize = bool(standardize)\n",
    "        self.random_state = random_state\n",
    "        self.verbose = verbose\n",
    "\n",
    "        # Learned params\n",
    "        self.coef_: Optional[np.ndarray] = None\n",
    "        self.intercept_: float = 0.0\n",
    "        self.loss_history_: list[float] = []\n",
    "\n",
    "        # Internal scaler for GD\n",
    "        self._scaler: Optional[_StandardScaler] = None\n",
    "\n",
    "    # ---- public API ----\n",
    "    def fit(self, X: np.ndarray, y: np.ndarray) -> \"LinearRegression\":\n",
    "        X, y = _check_X_y(X, y)\n",
    "        n_samples, n_features = X.shape\n",
    "\n",
    "        if self.solver == \"normal\":\n",
    "            b, w = self._fit_normal_equation(X, y)\n",
    "            self.intercept_ = b\n",
    "            self.coef_ = w\n",
    "            self.loss_history_.clear()\n",
    "            return self\n",
    "\n",
    "        # solver == \"gd\"\n",
    "        Xs = X\n",
    "        self._scaler = None\n",
    "        if self.standardize:\n",
    "            self._scaler = _StandardScaler().fit(X)\n",
    "            Xs = self._scaler.transform(X)\n",
    "\n",
    "        # Build design matrix\n",
    "        if self.fit_intercept:\n",
    "            Xb = np.c_[np.ones((n_samples, 1)), Xs]  # [1, x1, ..., xp]\n",
    "        else:\n",
    "            Xb = Xs\n",
    "\n",
    "        # Initialize weights\n",
    "        rng = np.random.default_rng(self.random_state)\n",
    "        w = rng.normal(loc=0.0, scale=0.01, size=(Xb.shape[1],))\n",
    "\n",
    "        self.loss_history_.clear()\n",
    "        prev_loss = np.inf\n",
    "\n",
    "        for it in range(self.n_iters):\n",
    "            y_pred = Xb @ w\n",
    "            resid = y_pred - y\n",
    "            loss = float(np.mean(resid ** 2))\n",
    "            # L2 regularization (do not penalize intercept)\n",
    "            if self.l2 > 0:\n",
    "                if self.fit_intercept:\n",
    "                    loss += self.l2 * float(np.sum(w[1:] ** 2)) / n_samples\n",
    "                else:\n",
    "                    loss += self.l2 * float(np.sum(w ** 2)) / n_samples\n",
    "\n",
    "            self.loss_history_.append(loss)\n",
    "\n",
    "            # Gradient\n",
    "            grad = (2.0 / n_samples) * (Xb.T @ resid)\n",
    "            if self.l2 > 0:\n",
    "                if self.fit_intercept:\n",
    "                    grad[1:] += (2.0 * self.l2 / n_samples) * w[1:]\n",
    "                else:\n",
    "                    grad += (2.0 * self.l2 / n_samples) * w\n",
    "\n",
    "            # Update\n",
    "            w -= self.learning_rate * grad\n",
    "\n",
    "            # Early stopping\n",
    "            if abs(prev_loss - loss) < self.tol:\n",
    "                if self.verbose:\n",
    "                    print(f\"[GD] Converged at iter={it}, loss={loss:.6f}\")\n",
    "                break\n",
    "            prev_loss = loss\n",
    "\n",
    "        if self.fit_intercept:\n",
    "            self.intercept_ = float(w[0])\n",
    "            self.coef_ = w[1:].copy()\n",
    "        else:\n",
    "            self.intercept_ = 0.0\n",
    "            self.coef_ = w.copy()\n",
    "\n",
    "        return self\n",
    "\n",
    "    def predict(self, X: np.ndarray) -> np.ndarray:\n",
    "        if self.coef_ is None:\n",
    "            raise RuntimeError(\"Model not fitted. Call fit(...) first.\")\n",
    "        X = np.asarray(X, dtype=float)\n",
    "        if X.ndim == 1:\n",
    "            X = X.reshape(-1, 1)\n",
    "\n",
    "        Xs = X\n",
    "        if self.solver == \"gd\" and self.standardize and self._scaler is not None:\n",
    "            Xs = self._scaler.transform(X)\n",
    "\n",
    "        return (self.intercept_ + Xs @ self.coef_).astype(float)\n",
    "\n",
    "    def score(self, X: np.ndarray, y: np.ndarray) -> float:\n",
    "        \"\"\"R² score.\"\"\"\n",
    "        return r2_score(y, self.predict(X))\n",
    "\n",
    "    # scikit-learn style helpers\n",
    "    def get_params(self, deep: bool = True) -> Dict[str, Any]:\n",
    "        return {\n",
    "            \"fit_intercept\": self.fit_intercept,\n",
    "            \"solver\": self.solver,\n",
    "            \"l2\": self.l2,\n",
    "            \"learning_rate\": self.learning_rate,\n",
    "            \"n_iters\": self.n_iters,\n",
    "            \"tol\": self.tol,\n",
    "            \"standardize\": self.standardize,\n",
    "            \"random_state\": self.random_state,\n",
    "            \"verbose\": self.verbose,\n",
    "        }\n",
    "\n",
    "    def set_params(self, **params: Any) -> \"LinearRegression\":\n",
    "        for k, v in params.items():\n",
    "            if not hasattr(self, k):\n",
    "                raise ValueError(f\"Unknown parameter: {k}\")\n",
    "            setattr(self, k, v)\n",
    "        return self\n",
    "      # ---- internal: normal equation ----\n",
    "    def _fit_normal_equation(self, X: np.ndarray, y: np.ndarray) -> Tuple[float, np.ndarray]:\n",
    "        \"\"\"\n",
    "        Closed-form solution:\n",
    "            w = (X^T X + λI)^(-1) X^T y\n",
    "        If fit_intercept=True, the bias column is included but NOT regularized.\n",
    "        \"\"\"\n",
    "        n_samples, n_features = X.shape\n",
    "        if self.fit_intercept:\n",
    "            Xb = np.c_[np.ones((n_samples, 1)), X]\n",
    "            n_params = n_features + 1\n",
    "        else:\n",
    "            Xb = X\n",
    "            n_params = n_features\n",
    "\n",
    "        I = np.eye(n_params)\n",
    "        if self.fit_intercept:\n",
    "            I[0, 0] = 0.0  # don't regularize intercept\n",
    "\n",
    "        reg = self.l2 * I\n",
    "        XtX = Xb.T @ Xb\n",
    "        Xty = Xb.T @ y\n",
    "\n",
    "        try:\n",
    "            w = np.linalg.solve(XtX + reg, Xty)\n",
    "        except np.linalg.LinAlgError:\n",
    "            # fallback to pseudo-inverse if singular\n",
    "            w = np.linalg.pinv(XtX + reg) @ Xty\n",
    "\n",
    "        if self.fit_intercept:\n",
    "            intercept = float(w[0])\n",
    "            coef = w[1:].astype(float)\n",
    "        else:\n",
    "            intercept = 0.0\n",
    "            coef = w.astype(float)\n",
    "\n",
    "        return intercept, coef\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "012c5a2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Normal Equation ===\n",
      "Intercept: -9999.388472194058\n",
      "Coefficients: [7569.87296439  163.54483411]\n",
      "R²: 0.9930264482849642\n",
      "\n",
      "=== Gradient Descent ===\n",
      "Intercept: 38657.142857142826\n",
      "Coefficients: [7798.16000786 6665.5360461 ]\n",
      "R²: 0.9930264482849632\n",
      "\n",
      "Predictions (NE): [37241.95553754 62194.39151294]\n",
      "Predictions (GD): [37241.95610448 62194.39139681]\n"
     ]
    }
   ],
   "source": [
    "# ---------- Example usage ----------\n",
    "if __name__ == \"__main__\":\n",
    "    # Small demo dataset: rooms, area -> price\n",
    "    data = {\n",
    "        \"h1\": [3, 120, 30000],\n",
    "        \"h2\": [2, 105, 21900],\n",
    "        \"h3\": [5, 200, 61200],\n",
    "        \"h4\": [4, 185, 49800],\n",
    "        \"h5\": [2, 100, 22700],\n",
    "        \"h6\": [3, 118, 33400],\n",
    "        \"h7\": [4, 190, 51600],\n",
    "    }\n",
    "    X = np.array([v[:2] for v in data.values()], dtype=float)\n",
    "    y = np.array([v[2] for v in data.values()], dtype=float)\n",
    "\n",
    "    # Normal equation (exact/closed-form)\n",
    "    model_ne = LinearRegression(solver=\"normal\", fit_intercept=True, l2=0.0)\n",
    "    model_ne.fit(X, y)\n",
    "    print(\"=== Normal Equation ===\")\n",
    "    print(\"Intercept:\", model_ne.intercept_)\n",
    "    print(\"Coefficients:\", model_ne.coef_)\n",
    "    print(\"R²:\", model_ne.score(X, y))\n",
    "\n",
    "    # Gradient descent (iterative) with standardization\n",
    "    model_gd = LinearRegression(\n",
    "        solver=\"gd\",\n",
    "        fit_intercept=True,\n",
    "        l2=0.0,\n",
    "        learning_rate=0.05,\n",
    "        n_iters=50_000,\n",
    "        tol=1e-10,\n",
    "        standardize=True,\n",
    "        random_state=0,\n",
    "        verbose=False,\n",
    "    )\n",
    "    model_gd.fit(X, y)\n",
    "    print(\"\\n=== Gradient Descent ===\")\n",
    "    print(\"Intercept:\", model_gd.intercept_)\n",
    "    print(\"Coefficients:\", model_gd.coef_)\n",
    "    print(\"R²:\", model_gd.score(X, y))\n",
    "\n",
    "    # Predictions\n",
    "    sample = np.array([[3, 150], [5, 210]], dtype=float)\n",
    "    print(\"\\nPredictions (NE):\", model_ne.predict(sample))\n",
    "    print(\"Predictions (GD):\", model_gd.predict(sample))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
