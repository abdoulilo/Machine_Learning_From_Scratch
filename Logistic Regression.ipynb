{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8650c636",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Logistic Regression from Scratch \n",
    "#\n",
    "# **Goal**: Implement binary logistic regression *from first principles*, mirroring the structure of your linear regression notebook.\n",
    "#\n",
    "# **What you'll learn**\n",
    "# - Model formulation: \\( z = w^ x + b \\), \\( \\hat y = \\sigma(z) \\)\n",
    "# - Loss: Binary cross-entropy (negative log-likelihood)\n",
    "# - Gradients & training with batch gradient descent\n",
    "# - Evaluation: accuracy, confusion matrix, and decision boundary (2 features)\n",
    "#\n",
    "# -\n",
    "\n",
    "# 1. Imports & Setup\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "np.set_printoptions(precision=4, suppress=True)\n",
    "RNG = np.random.default_rng(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4399931d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Synthetic Dataset\n",
    "#use the same structure you started with: two features and a binary label.\n",
    "# Data (your original values)\n",
    "data = {\n",
    "\"feature1\": np.array([1.2, 2.4, 3.1, 4.5, 5.0, 6.7, 7.2, 8.9, 9.5, 10.0], dtype=float),\n",
    "\"feature2\": np.array([0.5, 1.0, 1.2, 2.3, 2.8, 3.5, 4.1, 4.9, 5.5, 6.0], dtype=float),\n",
    "\"label\": np.array([0, 0, 0, 0, 1, 0, 1, 1, 1, 1], dtype=float),\n",
    "}\n",
    "\n",
    "\n",
    "X = np.column_stack([data[\"feature1\"], data[\"feature2\"]]) \n",
    "y = data[\"label\"] \n",
    "print(\"X shape:\", X.shape, \" y shape:\", y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e437b05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick Visualization\n",
    "\n",
    "plt.figure()\n",
    "plt.scatter(X[y==0, 0], X[y==0, 1], label=\"Class 0\", marker=\"o\")\n",
    "plt.scatter(X[y==1, 0], X[y==1, 1], label=\"Class 1\", marker=\"x\")\n",
    "plt.xlabel(\"feature1\")\n",
    "plt.ylabel(\"feature2\")\n",
    "plt.legend()\n",
    "plt.title(\"Synthetic Data\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccc1e8fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_test_split_np(X_mat: np.ndarray, y_vec: np.ndarray, train_ratio: float = 0.8):\n",
    "    m = len(y_vec)\n",
    "    nth = int(train_ratio * m)\n",
    "    return X_mat[:nth], y_vec[:nth], X_mat[nth:], y_vec[nth:]\n",
    "\n",
    "\n",
    "X_train_cols, y_train, X_test_cols, y_test = train_test_split_lists([X[:,0], X[:,1]], y, 0.8)\n",
    "X_train, y_train_np, X_test, y_test_np = train_test_split_np(X, y, 0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fbea949",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## 4. Model: Sigmoid, Linear Score, Probabilities, and Loss\n",
    "\n",
    "def sigmoid(z: float) -> float:\n",
    "    # Stable sigmoid for scalar z\n",
    "    # For vector use, we will rely on numpy later\n",
    "    return 1.0 / (1.0 + math.exp(-z))\n",
    "\n",
    "# Row-wise probabilities \n",
    "\n",
    "def y_hat(b: float, W: list, X_cols: list):\n",
    "    probs = []\n",
    "    for x1, x2 in zip(X_cols[0], X_cols[1]):\n",
    "        z = x1*W[0] + x2*W[1] + b\n",
    "        probs.append(sigmoid(z))\n",
    "    return probs\n",
    "\n",
    "# Vectorized versions (for loss/grad plots etc.)\n",
    "\n",
    "def predict_proba(X_mat: np.ndarray, W_vec: np.ndarray, b: float) -> np.ndarray:\n",
    "    z = X_mat @ W_vec + b\n",
    "    return 1.0 / (1.0 + np.exp(-z))\n",
    "\n",
    "\n",
    "def predict_labels(X_mat: np.ndarray, W_vec: np.ndarray, b: float, threshold: float = 0.5) -> np.ndarray:\n",
    "    proba = predict_proba(X_mat, W_vec, b)\n",
    "    return (proba >= threshold).astype(int)\n",
    "\n",
    "\n",
    "def binary_cross_entropy(y_true: np.ndarray, y_prob: np.ndarray) -> float:\n",
    "    eps = 1e-12\n",
    "    y_prob = np.clip(y_prob, eps, 1 - eps)\n",
    "    return -np.mean(y_true * np.log(y_prob) + (1 - y_true) * np.log(1 - y_prob))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10ab8b06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## 5. Gradients \n",
    "# Using your per-parameter gradient helpers and also a vectorized variant.\n",
    "\n",
    "def dj_dw(y_list: list, yhat_list: list, X_cols: list, n: int):\n",
    "    m = len(y_list)\n",
    "    total = 0.0\n",
    "    for i in range(m):\n",
    "        total += (yhat_list[i] - y_list[i]) * X_cols[n][i]\n",
    "    return total / m\n",
    "\n",
    "\n",
    "def dj_db(y_list: list, yhat_list: list):\n",
    "    m = len(y_list)\n",
    "    total = 0.0\n",
    "    for i in range(m):\n",
    "        total += (yhat_list[i] - y_list[i])\n",
    "    return total / m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d17e3ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorized gradients \n",
    "\n",
    "def gradients(X_mat: np.ndarray, y_vec: np.ndarray, W_vec: np.ndarray, b: float):\n",
    "    m = len(y_vec)\n",
    "    probs = predict_proba(X_mat, W_vec, b)\n",
    "    error = probs - y_vec\n",
    "    grad_W = (X_mat.T @ error) / m\n",
    "    grad_b = np.sum(error) / m\n",
    "    return grad_W, grad_b, probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc89167c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (B) Vectorized with history\n",
    "\n",
    "def train_vectorized(X_mat, y_vec, epochs=500, learn_rate=0.05):\n",
    "    W = np.zeros(X_mat.shape[1])\n",
    "    b = 0.0\n",
    "    history = {\"loss\": [], \"W\": [], \"b\": []}\n",
    "    for t in range(epochs):\n",
    "        grad_W, grad_b, probs = gradients(X_mat, y_vec, W, b)\n",
    "        # record loss\n",
    "        loss = binary_cross_entropy(y_vec, probs)\n",
    "        history[\"loss\"].append(loss)\n",
    "        history[\"W\"].append(W.copy())\n",
    "        history[\"b\"].append(b)\n",
    "        # update\n",
    "        W -= learn_rate * grad_W\n",
    "        b -= learn_rate * grad_b\n",
    "    return W, b, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "411e1b6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# B) vectorized trainer\n",
    "W_vec, b_vec, hist = train_vectorized(X_train, y_train_np, epochs=800, learn_rate=0.05)\n",
    "print(\"[Vectorized] W:\", W_vec, \" b:\", b_vec)\n",
    "\n",
    "# Evaluate on test set\n",
    "probs_test = predict_proba(X_test, W_vec, b_vec)\n",
    "preds_test = (probs_test >= 0.5).astype(int)\n",
    "acc_test = np.mean(preds_test == y_test_np.astype(int))\n",
    "print(\"Test accuracy:\", round(acc_test, 4))\n",
    "print(\"Test predictions:\", preds_test.tolist())\n",
    "print(\"Test true labels:\", y_test_np.astype(int).tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07956d3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.figure()\n",
    "plt.plot(hist[\"loss\"])\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Binary Cross-Entropy\")\n",
    "plt.title(\"Training Loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13360382",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def confusion_matrix(true, pred):\n",
    "    # returns (TP, FP, FN, TN)\n",
    "    tp = np.sum((true == 1) & (pred == 1))\n",
    "    tn = np.sum((true == 0) & (pred == 0))\n",
    "    fp = np.sum((true == 0) & (pred == 1))\n",
    "    fn = np.sum((true == 1) & (pred == 0))\n",
    "    return tp, fp, fn, tn\n",
    "\n",
    "TP, FP, FN, TN = confusion_matrix(y_test_np.astype(int), preds_test)\n",
    "print({\"TP\": int(TP), \"FP\": int(FP), \"FN\": int(FN), \"TN\": int(TN)})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
